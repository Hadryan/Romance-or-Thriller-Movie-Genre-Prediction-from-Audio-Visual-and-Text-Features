{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading_Data\n",
    "import pandas as pd\n",
    "def load_data():\n",
    "    #Concating both the valid data set and train to handel the data together\n",
    "    valid_features_1 = pd.read_csv('valid_features.tsv', sep='\\t')\n",
    "    train_features_1 = pd.read_csv('train_features.tsv', sep='\\t')\n",
    "    data_acc = pd.concat([train_features_1,valid_features_1]).reset_index(drop=True)\n",
    "    return data_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data for Audio and video dataset\n",
    "from sklearn import preprocessing\n",
    "def split_data_AV(X):\n",
    "    #getting the audio and video dataset [Dataset-3] from the dataframe\n",
    "    AV_train = load_data().iloc[0:5240,5:132]\n",
    "    AV_test = load_data().iloc[5240:5539,5:132]\n",
    "    #Splitting the dataset\n",
    "    if X == 'train_X':\n",
    "        return AV_train\n",
    "    elif X == 'test_X': \n",
    "        return AV_test\n",
    "    elif X == 'train_Y':\n",
    "        train_lable_1 = pd.read_csv('train_labels.tsv', sep='\\t')\n",
    "        lb_make = preprocessing.LabelEncoder()\n",
    "        train_lable_1['genres'] = lb_make.fit_transform(train_lable_1['genres'])\n",
    "        return train_lable_1\n",
    "    elif X == 'test_Y':\n",
    "        valid_labels = pd.read_csv('valid_labels.tsv', sep='\\t')\n",
    "        lb_make = preprocessing.LabelEncoder()\n",
    "        valid_labels['genres'] = lb_make.fit_transform(valid_labels['genres'])\n",
    "        return valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GB naive Bayes for audio and video dataset\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "def G_NB_AV(): \n",
    "    dummy_clf_AV = DummyClassifier()\n",
    "    #training the Audio and Video dataset [Dataset-3] using baseline model.\n",
    "    dummy_clf_AV.fit(split_data_AV('train_X'), split_data_AV('train_Y')['genres'])\n",
    "    #predicting the values\n",
    "    dummy_clf_AV.predict(split_data_AV('test_X'))\n",
    "    return dummy_clf_AV.predict(split_data_AV('test_X'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation for the gaussian NB for AV dataset \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "def eval_G_NB_AV():\n",
    "    #comparing the values using the above libraries for Dataset-3 [Audio and VIdeo features]\n",
    "    y_true = list(G_NB_AV())\n",
    "    y_pred = list(split_data_AV('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\dummy.py:132: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  \"stratified to prior in 0.24.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.18      0.21      0.20        33\n",
      "           5       0.00      0.00      0.00        16\n",
      "           6       0.00      0.00      0.00        10\n",
      "           7       0.12      0.12      0.12        43\n",
      "           8       0.06      0.04      0.05        24\n",
      "           9       0.00      0.00      0.00         6\n",
      "          10       0.00      0.00      0.00        10\n",
      "          11       0.00      0.00      0.00        10\n",
      "          12       0.00      0.00      0.00        15\n",
      "          13       0.18      0.23      0.20        40\n",
      "          14       0.12      0.08      0.10        24\n",
      "          15       0.18      0.17      0.17        30\n",
      "          16       0.00      0.00      0.00        15\n",
      "          17       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.10       299\n",
      "   macro avg       0.05      0.05      0.05       299\n",
      "weighted avg       0.09      0.10      0.09       299\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09698996655518395"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " eval_G_NB_AV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression for A&V dataset\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def LR_AV():\n",
    "    #Training and predicting the values using Logistic Regression Library for Audio and Video dataset [Dataset-3]\n",
    "    clf = LogisticRegression(random_state=0,max_iter=15000)\n",
    "    clf.fit(split_data_AV('train_X'), split_data_AV('train_Y')['genres'])\n",
    "    return clf.predict(split_data_AV('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation for the Logistic Regression for AV dataset \n",
    "from sklearn.metrics import accuracy_score\n",
    "def eval_LR_AV():\n",
    "    y_true = list(LR_AV())\n",
    "    y_pred = list(split_data_AV('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.33      0.50      0.40         2\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.50      0.37      0.43        51\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.39      0.41      0.40        17\n",
      "           7       0.30      0.25      0.28        51\n",
      "           8       0.17      0.21      0.19        14\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.25      0.18      0.21        11\n",
      "          11       0.00      0.00      0.00         6\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.39      0.38      0.38        53\n",
      "          14       0.31      0.26      0.29        19\n",
      "          15       0.57      0.25      0.35        63\n",
      "          16       0.10      0.40      0.15         5\n",
      "          17       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.29       299\n",
      "   macro avg       0.18      0.18      0.17       299\n",
      "weighted avg       0.39      0.29      0.32       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29431438127090304"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_LR_AV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier for A&V dataset\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "def SVM_AV():\n",
    "    #Training and predicting the values using SVM Library for Audio and Video dataset [Dataset-3]\n",
    "    clf_l_g_1 = svm.SVC(decision_function_shape='ovo')\n",
    "    clf_l_g_1.fit(split_data_AV('train_X'), split_data_AV('train_Y')['genres'])\n",
    "    return clf_l_g_1.predict(split_data_AV('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation for the SVM for AV dataset \n",
    "from sklearn.metrics import accuracy_score\n",
    "def eval_SVM_AV():\n",
    "    #comparing the values using the above libraries for Dataset-3 [Audio and VIdeo features]\n",
    "    y_true = list(SVM_AV())\n",
    "    y_pred = list(split_data_AV('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.88      0.19      0.31       242\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.32      0.17      0.22        54\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.18       299\n",
      "   macro avg       0.07      0.02      0.03       299\n",
      "weighted avg       0.77      0.18      0.29       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1806020066889632"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_SVM_AV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP classifier for A&V dataaset\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def MLP_AV():\n",
    "    #Training and predicting the values using MLPClassifier Library for Audio and Video dataset [Dataset-3]\n",
    "    mlp = MLPClassifier(activation='logistic',hidden_layer_sizes=(500,100),solver='sgd',learning_rate_init = 0.5,n_iter_no_change = 5)\n",
    "    mlp.fit(split_data_AV('train_X'), split_data_AV('train_Y')['genres'])\n",
    "    return mlp.predict(split_data_AV('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation for the MLP for AV dataset \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "def eval_MLP_AV():\n",
    "    y_true = list(MLP_AV())\n",
    "    y_pred = list(split_data_AV('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       1.00      0.17      0.29       299\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.17       299\n",
      "   macro avg       0.06      0.01      0.02       299\n",
      "weighted avg       1.00      0.17      0.29       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1705685618729097"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_MLP_AV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data for title and tag Dataset-2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "def split_data_word(X):\n",
    "    #vectorizing title feature of the data\n",
    "    vec_1= TfidfVectorizer(token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "    ti = vec_1.fit_transform(load_data()['title'].values.astype('U'))\n",
    "    title_ds = pd.DataFrame(ti.toarray(), columns=vec_1.get_feature_names())\n",
    "    #vectorizing the tag feature of the data\n",
    "    vec_2 = TfidfVectorizer(token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "    ta = vec_2.fit_transform(load_data()['tag'].values.astype('U'))\n",
    "    tag_ds = pd.DataFrame(ta.toarray(), columns=vec_2.get_feature_names())\n",
    "    #concating both the features in one dataset\n",
    "    tag_col = list(tag_ds.columns)\n",
    "    for x in range(len(tag_col)):\n",
    "        title_ds[tag_col[x]]=tag_ds[tag_col[x]].values\n",
    "    #splitting the dataset into train and test\n",
    "    if X == 'train_X':\n",
    "        train_word_1 =  title_ds.iloc[0:5240,:]\n",
    "        return train_word_1\n",
    "    elif X == 'test_X':\n",
    "        test_word_1 =  title_ds.iloc[5240:5539,:]\n",
    "        return test_word_1\n",
    "    elif X == 'train_Y':\n",
    "        train_lable_1 = pd.read_csv('train_labels.tsv', sep='\\t')\n",
    "        lb_make = preprocessing.LabelEncoder()\n",
    "        train_lable_1['genres'] = lb_make.fit_transform(train_lable_1['genres'])\n",
    "        return train_lable_1\n",
    "    elif X == 'test_Y':\n",
    "        valid_labels = pd.read_csv('valid_labels.tsv', sep='\\t')\n",
    "        lb_make = preprocessing.LabelEncoder()\n",
    "        valid_labels['genres'] = lb_make.fit_transform(valid_labels['genres'])\n",
    "        return valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GB naive Bayes for title and tag dataset\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "def G_NB_W(): \n",
    "    #Training and predicting the values using Baseline Model Library for Title and Tag dataset [Dataset-2]\n",
    "    dummy_clf_W = DummyClassifier()\n",
    "    #training the dataset\n",
    "    dummy_clf_W.fit(split_data_word('train_X'), split_data_word('train_Y')['genres'])\n",
    "    #predicting the values\n",
    "    return dummy_clf_W.predict(split_data_word('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation for the gaussian NB for title and tag dataset \n",
    "\n",
    "def eval_G_NB_W():\n",
    "    y_true = list(G_NB_W())\n",
    "    y_pred = list(split_data_word('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\dummy.py:132: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  \"stratified to prior in 0.24.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.00      0.00      0.00        10\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         8\n",
      "           4       0.11      0.11      0.11        37\n",
      "           5       0.00      0.00      0.00        15\n",
      "           6       0.00      0.00      0.00         6\n",
      "           7       0.12      0.15      0.13        34\n",
      "           8       0.17      0.14      0.15        22\n",
      "           9       0.00      0.00      0.00         7\n",
      "          10       0.00      0.00      0.00        10\n",
      "          11       0.00      0.00      0.00         7\n",
      "          12       0.06      0.04      0.04        28\n",
      "          13       0.22      0.27      0.24        41\n",
      "          14       0.12      0.11      0.11        19\n",
      "          15       0.11      0.10      0.11        29\n",
      "          16       0.00      0.00      0.00        13\n",
      "          17       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.10       299\n",
      "   macro avg       0.05      0.05      0.05       299\n",
      "weighted avg       0.09      0.10      0.09       299\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09698996655518395"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_G_NB_W()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression for title and tag dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def LR_W():\n",
    "    #Training and predicting the values using Logistic Regression Library for Title and Tag dataset [Dataset-2]\n",
    "    clf_w = LogisticRegression(random_state=0,max_iter=15000)\n",
    "    clf_w.fit(split_data_word('train_X'), split_data_word('train_Y')['genres'])\n",
    "    return clf_w.predict(split_data_word('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation for the Logistic Regression for AV dataset \n",
    "\n",
    "def eval_LR_W():\n",
    "    y_true = list(LR_W())\n",
    "    y_pred = list(split_data_word('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.37      0.40      0.38        35\n",
      "           5       0.20      0.25      0.22         4\n",
      "           6       0.39      0.54      0.45        13\n",
      "           7       0.67      0.31      0.43        93\n",
      "           8       0.22      0.50      0.31         8\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.50      0.44      0.47         9\n",
      "          11       0.10      0.25      0.14         4\n",
      "          12       0.11      0.67      0.19         3\n",
      "          13       0.43      0.32      0.37        69\n",
      "          14       0.75      0.57      0.65        21\n",
      "          15       0.39      0.33      0.36        33\n",
      "          16       0.24      0.83      0.37         6\n",
      "          17       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.37       299\n",
      "   macro avg       0.24      0.30      0.24       299\n",
      "weighted avg       0.50      0.37      0.40       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3745819397993311"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_LR_W()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier for title and tag dataset\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "def SVM_W():\n",
    "    #Training and predicting the values using SVM Library for Title and Tag dataset [Dataset-2]\n",
    "    clf_l_g_1_w = svm.SVC(decision_function_shape='ovo')\n",
    "    clf_l_g_1_w.fit(split_data_word('train_X'), split_data_word('train_Y')['genres'])\n",
    "    return clf_l_g_1_w.predict(split_data_word('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation for the SVM for title and tag dataset \n",
    "def eval_SVM_W():\n",
    "    y_true = list(SVM_W())\n",
    "    y_pred = list(split_data_word('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.37      0.45      0.41        31\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.44      0.62      0.52        13\n",
      "           7       0.65      0.30      0.41        94\n",
      "           8       0.28      0.56      0.37         9\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.38      0.43      0.40         7\n",
      "          11       0.10      0.20      0.13         5\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.47      0.29      0.36        82\n",
      "          14       0.69      0.65      0.67        17\n",
      "          15       0.46      0.38      0.42        34\n",
      "          16       0.24      0.71      0.36         7\n",
      "          17       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.37       299\n",
      "   macro avg       0.23      0.25      0.22       299\n",
      "weighted avg       0.51      0.37      0.41       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3745819397993311"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_SVM_W()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP classifier for word dataaset\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def MLP_W():\n",
    "    #Training and predicting the values using MLP Library for Title and Tag dataset [Dataset-2]\n",
    "    mlp_1 = MLPClassifier(activation='logistic',hidden_layer_sizes=(500,100),solver='sgd',learning_rate_init = 0.5,n_iter_no_change = 5)\n",
    "    mlp_1.fit(split_data_word('train_X'), split_data_word('train_Y')['genres'])\n",
    "    return mlp_1.predict(split_data_word('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_MLP_W():\n",
    "    y_true = list(MLP_W())\n",
    "    y_pred = list(split_data_word('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       1.00      0.17      0.29       299\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.17       299\n",
      "   macro avg       0.06      0.01      0.02       299\n",
      "weighted avg       1.00      0.17      0.29       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1705685618729097"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_MLP_W()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "def split_data(X):\n",
    "    #combing all the features in one single table for comparision Title,Tag,Video and Audio Features.\n",
    "    AV_train = load_data().iloc[0:5240,5:132]\n",
    "    AV_test = load_data().iloc[5240:5539,5:132]\n",
    "    #title and tag dataset\n",
    "    vec_1= TfidfVectorizer(token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "    ti = vec_1.fit_transform(load_data()['title'].values.astype('U'))\n",
    "    title_ds = pd.DataFrame(ti.toarray(), columns=vec_1.get_feature_names())\n",
    "    #vectorizing the tag feature of the data\n",
    "    vec_2 = TfidfVectorizer(token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "    ta = vec_2.fit_transform(load_data()['tag'].values.astype('U'))\n",
    "    tag_ds = pd.DataFrame(ta.toarray(), columns=vec_2.get_feature_names())\n",
    "    #concating both the features in one dataset\n",
    "    tag_col = list(tag_ds.columns)\n",
    "    for x in range(len(tag_col)):\n",
    "        title_ds[tag_col[x]]=tag_ds[tag_col[x]].values\n",
    "\n",
    "    train_word_1 =  title_ds.iloc[0:5240,:]\n",
    "    test_word_1 =  title_ds.iloc[5240:5539,:]\n",
    "\n",
    "    '''combing the dataset to one'''\n",
    "\n",
    "    #vectorizing title feature of the data\n",
    "   \n",
    "    #splitting the dataset into train and test\n",
    "    if X == 'train_X':\n",
    "        AV_train_col = list(AV_train.columns)\n",
    "        final_train_dataset = title_ds.iloc[0:5240,:]\n",
    "        for x in range(len(AV_train_col)):\n",
    "            final_train_dataset[AV_train_col[x]]=AV_train[AV_train_col[x]].values\n",
    "        return final_train_dataset\n",
    "    elif X == 'test_X':\n",
    "        AV_test_col = list(AV_test.columns)\n",
    "        final_test_dataset = title_ds.iloc[5240:5539,:]\n",
    "        for x in range(len(AV_test_col)):\n",
    "            final_test_dataset[AV_test_col[x]]=AV_test[AV_test_col[x]].values       \n",
    "        return final_test_dataset\n",
    "    elif X == 'train_Y':\n",
    "        train_lable_1 = pd.read_csv('train_labels.tsv', sep='\\t')\n",
    "        lb_make = preprocessing.LabelEncoder()\n",
    "        train_lable_1['genres'] = lb_make.fit_transform(train_lable_1['genres'])\n",
    "        return train_lable_1\n",
    "    elif X == 'test_Y':\n",
    "        valid_labels = pd.read_csv('valid_labels.tsv', sep='\\t')\n",
    "        lb_make = preprocessing.LabelEncoder()\n",
    "        valid_labels['genres'] = lb_make.fit_transform(valid_labels['genres'])\n",
    "        return valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "def G_NB(): \n",
    "    #Training and predicting the values using Baseline Model Library for Whole dataset [Dataset-1]\n",
    "    dummy_clf = DummyClassifier(strategy='uniform')\n",
    "    #training the dataset\n",
    "    dummy_clf.fit(split_data('train_X'), split_data('train_Y')['genres'])\n",
    "    #predicting the values\n",
    "    return dummy_clf.predict(split_data('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation for the gaussian NB for entier dataset \n",
    "from sklearn.metrics import accuracy_score\n",
    "def eval_G_NB():\n",
    "    y_true = list(G_NB())\n",
    "    y_pred = list(split_data('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.07      0.10        15\n",
      "           1       0.00      0.00      0.00        20\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.08      0.20      0.11        15\n",
      "           5       0.00      0.00      0.00        20\n",
      "           6       0.00      0.00      0.00        15\n",
      "           7       0.00      0.00      0.00        15\n",
      "           8       0.00      0.00      0.00        11\n",
      "           9       0.25      0.11      0.15         9\n",
      "          10       0.12      0.05      0.07        19\n",
      "          11       0.00      0.00      0.00        22\n",
      "          12       0.17      0.14      0.15        22\n",
      "          13       0.04      0.14      0.06        14\n",
      "          14       0.06      0.05      0.06        19\n",
      "          15       0.14      0.24      0.18        17\n",
      "          16       0.10      0.10      0.10        21\n",
      "          17       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.06       299\n",
      "   macro avg       0.06      0.06      0.05       299\n",
      "weighted avg       0.06      0.06      0.05       299\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06020066889632107"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_G_NB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression for entier dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def LR():\n",
    "    #Training and predicting the values using Logistic Regression Library for Whole dataset [Dataset-1]\n",
    "    clf = LogisticRegression(random_state=0,max_iter=15000)\n",
    "    clf.fit(split_data('train_X'), split_data('train_Y')['genres'])\n",
    "    return clf.predict(split_data('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation for the Logistic Regression for entier dataset \n",
    "def eval_LR():\n",
    "    y_true = list(LR())\n",
    "    y_pred = list(split_data('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.33      1.00      0.50         1\n",
      "           3       0.33      0.25      0.29         4\n",
      "           4       0.45      0.40      0.43        42\n",
      "           5       0.20      0.33      0.25         3\n",
      "           6       0.50      0.43      0.46        21\n",
      "           7       0.65      0.39      0.49        72\n",
      "           8       0.28      0.45      0.34        11\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.75      0.46      0.57        13\n",
      "          11       0.20      0.33      0.25         6\n",
      "          12       0.11      1.00      0.20         2\n",
      "          13       0.45      0.44      0.45        52\n",
      "          14       0.62      0.56      0.59        18\n",
      "          15       0.61      0.42      0.50        40\n",
      "          16       0.24      0.56      0.33         9\n",
      "          17       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.42       299\n",
      "   macro avg       0.32      0.39      0.31       299\n",
      "weighted avg       0.51      0.42      0.45       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42474916387959866"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_LR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier for entier dataset\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "def SVM():\n",
    "    #Training and predicting the values using SVM Library for Whole dataset [Dataset-1]\n",
    "    clf_l_g_1 = svm.SVC(decision_function_shape='ovo')\n",
    "    clf_l_g_1.fit(split_data('train_X'), split_data('train_Y')['genres'])\n",
    "    return clf_l_g_1.predict(split_data('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation for the SVM for entier dataset \n",
    "def eval_SVM():\n",
    "    y_true = list(SVM())\n",
    "    y_pred = list(split_data('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.88      0.18      0.31       244\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.32      0.17      0.22        54\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.18       299\n",
      "   macro avg       0.07      0.02      0.03       299\n",
      "weighted avg       0.78      0.18      0.29       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1806020066889632"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP classifier for entier dataaset\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def MLP():\n",
    "    #Training and predicting the values using MLP Library for Whole dataset [Dataset-1]\n",
    "    mlp = MLPClassifier(activation='logistic',hidden_layer_sizes=(500,100),solver='sgd',learning_rate_init = 0.5,n_iter_no_change = 5)\n",
    "    mlp.fit(split_data('train_X'), split_data('train_Y')['genres'])\n",
    "    return mlp.predict(split_data('test_X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_MLP():\n",
    "    y_true = list(MLP())\n",
    "    y_pred = list(split_data_word('test_Y')['genres'].values)\n",
    "    print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]))\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       1.00      0.17      0.29       299\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.17       299\n",
      "   macro avg       0.06      0.01      0.02       299\n",
      "weighted avg       1.00      0.17      0.29       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1705685618729097"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
